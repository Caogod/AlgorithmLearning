{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import  torch.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    :param d_model: diamsion of word vector\n",
    "    :param vocab: size of vocabary table \n",
    "    :return\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # Use Embedding module of pytorch\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Get a word vector projected by Embedding layer\n",
    "        :param x: one-hot encode vector of word tokens\n",
    "        \"\"\"\n",
    "        # multiplication of diamsion will be help to make train stable (scale the gradient)\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Add positional encoding on word embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, d_model, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # regisier buffer will refuse the pe changed in backward\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:,:x.size(1)].clone().detach() # use add method combined word embedding and positional encoder\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, scale_factor, dropout=0.0) -> None:\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q / self.scale_factor, k.transpose(2,3))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.0) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(scale_factor=math.sqrt(d_k))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bz, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "        residual = q\n",
    "\n",
    "        q = self.norm(q)\n",
    "        k = self.norm(k)\n",
    "        v = self.norm(v)\n",
    "\n",
    "        # split head\n",
    "        q=self.w_qs(q)\n",
    "        q = q.view(bz, len_q, self.n_head, self.d_k).transpose(1,2)\n",
    "        k=self.w_ks(k)\n",
    "        k = k.view(bz, len_k, self.n_head, self.d_k).transpose(1,2)\n",
    "        v=self.w_vs(v)\n",
    "        v = v.view(bz, len_v, self.n_head, self.d_v).transpose(1,2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        q, attn = self.attn(q, k, v, mask)\n",
    "\n",
    "        # combine head\n",
    "        q = q.transpose(1,2).contiguous()\n",
    "        q = q.view(bz, len_q, -1)\n",
    "        q = self.fc(q)\n",
    "        q = self.dropout(q)\n",
    "        q = self.norm(q+residual) # residual\n",
    "        return q, attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12) -> None:\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        out = (x-mean) / torch.sqrt(var+self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model) -> None:\n",
    "        super(PoswiseFeedForward, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model,d_model,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model,d_model,bias=False)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = self.norm(output + x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atten_pad_mask(seq_q, seq_k):\n",
    "    bz, len_q = seq_q.size()\n",
    "    bz, len_k = seq_k.size()\n",
    "    pad_atten_mask = seq_k.data.eq(0)\n",
    "    return pad_atten_mask.expand(bz, len_q, len_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.selfattn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout)\n",
    "        self.feedward = PoswiseFeedForward(d_model)\n",
    "\n",
    "    def forward(self, inputs, input_masks):\n",
    "        output, attn = self.selfattn(inputs, inputs, inputs, input_masks)\n",
    "        output = self.feedward(output)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, n_head, n_layers, dropout) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = Embeddings(d_model, vocab)\n",
    "        self.pos_emb = PositionalEncoding(d_model, dropout)\n",
    "        self.layers = nn.ModuleList(EncoderLayer(n_head, d_model, d_model, d_model, dropout) for _ in range(n_layers))\n",
    "\n",
    "    def forward(self, inputs, masks=None):\n",
    "        embds = self.emb(inputs)\n",
    "        output = self.pos_emb(embds)\n",
    "        if masks is None:\n",
    "            masks = get_atten_pad_mask(embds, embds)\n",
    "        self_attns = []\n",
    "        for layer in self.layers:\n",
    "            output, attn = layer(output, masks)\n",
    "            self_attns.append(attn)\n",
    "        return output, self_attns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_decode_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    mask = torch.from_numpy(mask).byte()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1) -> None:\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.selfattn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout)\n",
    "        self.feedward = PoswiseFeedForward(d_model)\n",
    "\n",
    "    def forward(self, decode_input, encode_output, decode_mask, encode_mask):\n",
    "        decode_output, dec_attn = self.selfattn(decode_input, decode_input, decode_input, decode_mask)\n",
    "\n",
    "        decode_output, dec_enc_attn = self.cross_attn(decode_output, encode_output, encode_output, encode_mask)\n",
    "\n",
    "        decode_output = self.feedward(decode_output)\n",
    "        return decode_output, dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, n_head, n_layers, dropout) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = Embeddings(d_model, vocab)\n",
    "        self.pos_emb = PositionalEncoding(d_model, dropout)\n",
    "        self.layers = nn.ModuleList(DecoderLayer(n_head, d_model, d_model, d_model, dropout) for _ in range(n_layers))\n",
    "\n",
    "    def forward(self, decode_input, encode_input, encode_output):\n",
    "        decode_input = self.emb(decode_input)\n",
    "        output = self.pos_emb(decode_input)\n",
    "        decode_mask = get_train_decode_mask(decode_input)\n",
    "        decode_pad_mask = get_atten_pad_mask(decode_input)\n",
    "        decode_attn_mask = torch.gt((decode_mask+decode_pad_mask), 0)\n",
    "        cross_mask = get_atten_pad_mask(decode_input, encode_input)\n",
    "\n",
    "        decode_self_attn, decode_cross_attn = [], []\n",
    "        for layer in self.layers:\n",
    "            output, self_attn, cross_attn = layer(output, encode_output, decode_attn_mask, cross_mask)\n",
    "            decode_self_attn.append(self_attn)\n",
    "            decode_cross_attn.append(cross_attn)\n",
    "\n",
    "        return output, decode_self_attn, decode_cross_attn\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab, d_model, n_head, en_n_layers, de_n_layers, dropout=0.1) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(vocab, d_model, n_head, en_n_layers, dropout)\n",
    "\n",
    "        self.decoder = Decoder(vocab, d_model, n_head, de_n_layers, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab, bias=False)\n",
    "\n",
    "    def forward(self, encode_inputs, decode_inputs):\n",
    "        \n",
    "        encode_outputs, encode_attn = self.encoder(encode_inputs)\n",
    "\n",
    "        decode_outputs, decode_self_attn, decode_cross_attn = self.decoder(decode_inputs, encode_inputs, encode_outputs)\n",
    "\n",
    "        output = self.fc(decode_outputs)\n",
    "\n",
    "        logits = torch.softmax(output, dim=-1)\n",
    "\n",
    "        return output, logits, encode_attn, decode_self_attn, decode_cross_attn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
